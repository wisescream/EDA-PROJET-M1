{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de Sentiments - Critiques de Films\n",
    "\n",
    "Ce notebook couvre l'int√©gralit√© du pipeline d'analyse de sentiments sur le dataset Allocin√© :\n",
    "1.  **Collecte des donn√©es**\n",
    "2.  **Pr√©traitement (Cleaning & Lemmatisation)**\n",
    "3.  **Analyse Exploratoire (EDA)**\n",
    "4.  **Mod√©lisation (Naive Bayes, SVM, Random Forest)**\n",
    "5.  **√âvaluation**\n",
    "6.  **Interpr√©tation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import tqdm\n",
    "\n",
    "# Configuration visuelle\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collecte des donn√©es\n",
    "R√©cup√©ration du dataset `allocine` via Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chargement du dataset Allocin√©...\")\n",
    "try:\n",
    "    # On essaie de charger localement si d√©j√† t√©l√©charg√© par le script pr√©c√©dent\n",
    "    df = pd.read_csv(\"allocine_raw.csv\")\n",
    "    print(\"Donn√©es charg√©es depuis allocine_raw.csv\")\n",
    "except FileNotFoundError:\n",
    "    dataset = load_dataset(\"allocine\")\n",
    "    train_df = pd.DataFrame(dataset['train'])\n",
    "    val_df = pd.DataFrame(dataset['validation'])\n",
    "    test_df = pd.DataFrame(dataset['test'])\n",
    "    df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "    # Sauvegarde pour la prochaine fois\n",
    "    df = df[['review', 'label']]\n",
    "    df.to_csv(\"allocine_raw.csv\", index=False)\n",
    "    print(\"Donn√©es t√©l√©charg√©es et sauvegard√©es.\")\n",
    "\n",
    "print(f\"Nombre total d'avis : {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pr√©traitement (Cleaning)\n",
    "Objectifs :\n",
    "- Minuscules\n",
    "- Retrait HTML\n",
    "- Gestion Emojis\n",
    "- Lemmatisation avec SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de SpaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Mod√®le non trouv√©, t√©l√©chargement...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"fr_core_news_sm\")\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "EMOJI_MAPPING = {\n",
    "    \"ü§©\": \"g√©nial\", \"üòç\": \"ador√©\", \"‚ù§Ô∏è\": \"amour\", \"üëç\": \"bien\", \n",
    "    \"üëé\": \"nul\", \"üò°\": \"col√®re\", \"üòÇ\": \"rire\", \"üò≠\": \"triste\", \n",
    "    \"ü§¢\": \"d√©go√ªt\", \"ü§Æ\": \"horrible\", \"üëè\": \"bravo\", \"üî•\": \"feu\", \n",
    "    \"‚ú®\": \"brillant\", \"üíÄ\": \"mort\", \"üí©\": \"merde\"\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # Emojis\n",
    "    for emoji_char, text_replacement in EMOJI_MAPPING.items():\n",
    "        text = text.replace(emoji_char, f\" {text_replacement} \")\n",
    "    # HTML\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # Minuscules et espaces superflus\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "print(\"Nettoyage initial...\")\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Pour l'exercice, si le dataset est tr√®s grand, on lemmatise un √©chantillon \n",
    "# pour que l'ex√©cution ne prenne pas des heures (Optionnel: retirer .sample() pour tout faire)\n",
    "if len(df) > 5000:\n",
    "    print(\"Sous-√©chantillonnage √† 5000 avis pour la rapidit√© du notebook (Dataset complet dispo).\")\n",
    "    df_sample = df.sample(5000, random_state=42).copy()\n",
    "else:\n",
    "    df_sample = df.copy()\n",
    "\n",
    "print(\"Lemmatisation avec SpaCy...\")\n",
    "docs = list(nlp.pipe(df_sample['cleaned_review'], batch_size=50, disable=['ner', 'parser']))\n",
    "df_sample['lemmatized'] = [\" \".join([t.lemma_ for t in doc if not t.is_punct and not t.is_space]) for doc in docs]\n",
    "\n",
    "print(df_sample[['cleaned_review', 'lemmatized']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse Exploratoire (EDA)\n",
    "- Wordclouds (Positif vs N√©gatif)\n",
    "- Distribution des longueurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration par label (0: N√©gatif, 1: Positif chez Allocin√©)\n",
    "pos_reviews = df_sample[df_sample['label'] == 1]['lemmatized'].str.cat(sep=' ')\n",
    "neg_reviews = df_sample[df_sample['label'] == 0]['lemmatized'].str.cat(sep=' ')\n",
    "\n",
    "# Wordclouds\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "wc_pos = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(pos_reviews)\n",
    "plt.imshow(wc_pos, interpolation='bilinear')\n",
    "plt.title(\"Mots fr√©quents - Avis POSITIFS\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "wc_neg = WordCloud(width=800, height=400, background_color='black', colormap='magma').generate(neg_reviews)\n",
    "plt.imshow(wc_neg, interpolation='bilinear')\n",
    "plt.title(\"Mots fr√©quents - Avis N√âGATIFS\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Histogramme des longueurs\n",
    "df_sample['length'] = df_sample['cleaned_review'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df_sample, x='length', hue='label', bins=50, kde=True, palette='coolwarm')\n",
    "plt.title(\"Distribution de la longueur des avis (caract√®res)\")\n",
    "plt.xlabel(\"Longueur\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mod√©lisation\n",
    "Comparaison de 3 mod√®les : Naive Bayes, SVM (LinearSVC), Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.9)\n",
    "X = vectorizer.fit_transform(df_sample['lemmatized'])\n",
    "y = df_sample['label']\n",
    "\n",
    "# Split Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"SVM Lin√©aire\": LinearSVC(dual='auto', random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Entra√Ænement de {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results[name] = {\"model\": model, \"f1\": f1, \"pred\": y_pred}\n",
    "    print(f\"F1-Score ({name}) : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. √âvaluation\n",
    "D√©tail des performances pour le meilleur mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = max(results, key=lambda k: results[k]['f1'])\n",
    "print(f\"Meilleur mod√®le : {best_model_name}\")\n",
    "\n",
    "y_pred_best = results[best_model_name]['pred']\n",
    "\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f\"Matrice de Confusion ({best_model_name})\")\n",
    "plt.ylabel('Vrai Label')\n",
    "plt.xlabel('Label Pr√©dit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpr√©tation et Discussion\n",
    "Analyse des mots les plus influents (coefficients) pour le mod√®le lin√©aire (SVM ou RegLog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si le SVM est le meilleur ou disponible, on regarde ses coefs\n",
    "item_model = results.get(\"SVM Lin√©aire\", {}).get(\"model\")\n",
    "if item_model:\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coefs = item_model.coef_[0]\n",
    "    \n",
    "    # Top 10 mots positifs et n√©gatifs\n",
    "    top_pos_indices = coefs.argsort()[-10:][::-1]\n",
    "    top_neg_indices = coefs.argsort()[:10]\n",
    "    \n",
    "    print(\"Top 10 mots influen√ßant le POSITIF :\")\n",
    "    print([feature_names[i] for i in top_pos_indices])\n",
    "    \n",
    "    print(\"\\nTop 10 mots influen√ßant le N√âGATIF :\")\n",
    "    print([feature_names[i] for i in top_neg_indices])\n",
    "else:\n",
    "    print(\"Le SVM n'a pas √©t√© entra√Æn√©, impossible d'extraire les coefficients simplement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion des limites\n",
    "- **Ironie/Sarcasme** : Les mod√®les Bag-of-Words (comme TF-IDF) perdent l'ordre des mots et peinent √† d√©tecter l'ironie (\"Quel chef d'oeuvre... je me suis endormi\").\n",
    "- **Contexte** : \"Pas mal\" vs \"Vraiment mal\". La n√©gation est difficile √† capter sans n-grams ou mod√®les contextuels.\n",
    "- **Am√©lioration possible** : Utiliser BERT (Transformers) qui capte le contexte bidirectionnel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bonus : Deep Learning avec CamemBERT\n",
    "Utilisation d'un mod√®le Transformer pr√©-entra√Æn√© pour le fran√ßais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances pour le Deep Learning\n",
    "# Ex√©cutez cette cellule pour installer torch et transformers dans l'environnement du notebook\n",
    "%pip install torch transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# V√©rification GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device : {device}\")\n",
    "\n",
    "# Chargement Tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_len=128):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Pr√©paration des datasets PyTorch\n",
    "# On utilise les textes bruts (ou l√©g√®rement nettoy√©s) pour BERT, il g√®re bien le contexte\n",
    "# On r√©duit la taille pour la rapidit√© si pas de GPU\n",
    "if device.type == 'cpu':\n",
    "    print(\"Attention : Entra√Ænement sur CPU. Utilisation d'un sous-ensemble r√©duit (200 avis) pour la d√©mo.\")\n",
    "    sample_size = 200\n",
    "else:\n",
    "    sample_size = 2000 # Plus grand si GPU\n",
    "\n",
    "df_bert = df.sample(sample_size, random_state=42)\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(df_bert['review'], df_bert['label'], test_size=0.2)\n",
    "\n",
    "train_dataset = ReviewsDataset(X_train_bert.to_numpy(), y_train_bert.to_numpy(), tokenizer)\n",
    "test_dataset = ReviewsDataset(X_test_bert.to_numpy(), y_test_bert.to_numpy(), tokenizer)\n",
    "\n",
    "print(\"Datasets pr√™ts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du mod√®le\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Configuration de l'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,              # Peu d'√©poques pour la d√©mo\n",
    "    per_device_train_batch_size=8 if device.type == 'cpu' else 16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1,\n",
    "        'precision': precision\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"D√©but de l'entra√Ænement CamemBERT...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation finale BERT\n",
    "results_bert = trainer.evaluate()\n",
    "print(\"R√©sultats CamemBERT :\")\n",
    "print(results_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
